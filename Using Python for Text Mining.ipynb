{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellow defination and codes come from my final thesis in Lamar university. the original file is located in below location:\n",
    "#### https://books.google.com/books/about/Using_Python_to_Analyze_Safety_Records.html?id=b7sJkAEACAAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Python for Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These codes develope a method for analyzing and classifying text documents. \n",
    "#### I appled several classifier from Sci-kit Learn. I also determined classification accuracy and examine execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process of text mining consist of 3 steps. \n",
    "#### 1- Pre-Processing: The text document is convert to a numeric representation. Often stemming and stop word removal are used to reduce dimension.\n",
    "#### 2-\tData mining: Classifies numeric representation of the text.\n",
    "#### 3- Results validation: accuracy of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file\n",
    "import csv\n",
    "with open('C:\\\\Users\\\\lpouresmaeil\\\\Desktop\\\\near_miss.csv','r') as f:\n",
    "         reader = csv.reader(f)\n",
    "         description = []\n",
    "         for row in reader:   #encode them until can be tokenize\n",
    "           description.append(row[1].decode(encoding='UTF-8',errors='replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the format to array\n",
    "import numpy as np\n",
    "description_start = np.array(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test and train data set\n",
    "from sklearn import cross_validation\n",
    "rs = cross_validation.ShuffleSplit(len(description_start), n_iter=20,test_size=.1, random_state=0)\n",
    "for train, test in rs: \n",
    "    description_train = description_start [ train] \n",
    "    description_test = description_start [ test] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop works: Some common stop words are \"a\", \"an\", \"the\", \"as\", \"be\".\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenization:  breaking up a sequence of strings into pieces such as words,\n",
    "# keywords, phrases, symbols and other elements called tokens\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# Stemming: Remiving \"s\", \"es\", \"ing\", \"ed\" from the text.\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "new_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\").stem\n",
    "for i in range(len(description_train)):\n",
    "    tokens = tokenizer.tokenize(description_train[i])\n",
    "    good_words = [w for w in tokens if w not in stop]\n",
    "    stemmed_words = [stemmer(t) for t in good_words]\n",
    "    new_description.append( \" \".join(stemmed_words))\n",
    "description_train = new_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram :N-gram: continuance sequence of n items in a sentences. Unigram is the n-gram of size one, \n",
    "# bigram is the n-gram of size two and trigram is the n-gram of size three. \n",
    "# Trigram is more common in text mining.\n",
    "\n",
    "from nltk.util import ngrams\n",
    "description_ngrams = ngrams(description_train, 1, pad_left=True, pad_right=True, pad_symbol=' ')\n",
    "description_ngrams= list(description_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram function will return a tuple so we need to join positions in ngrams itself:\n",
    "new_grams= []\n",
    "for j in range(len(description_train)): \n",
    "    new_grams.append(''.join(description_ngrams[j]))\n",
    "description_train= new_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize: the process of converting text into numerical representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(description_train)\n",
    "X_test_counts = count_vect.transform(description_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: \n",
    "#### The basic idea comes from the K nearest document that are around the category of a given query in the document space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location_level_1_train is the other column in dataset. didn't show it here. (i didn't add all the xcolumns in these codes)\n",
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train_counts, location_level_1_train)\n",
    "# check the accuracy:\n",
    "clf.score(X_test_counts, location_level_1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayse:\n",
    "#### The Naïve Bayes classiﬁer is a simple classifier that can be very accurate. Naïve Bayes classifier has a statistical method where the frequency of the words makes this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNBB \n",
    "NB = MultinomialNB()BNB.fit(X_train_counts, location_level_1_train)\n",
    "predicted=BNB.predict(X_test_counts)\n",
    "answer2=BNB.predict_proba(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:\n",
    "#### Random forest is the combination of decision tress where all the decision tress have the same distribution. This algorithm breaks down data set to small subsets that are fit with decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_counts, location_level_1_train)\n",
    "rf.score(X_test_counts, location_level_1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
